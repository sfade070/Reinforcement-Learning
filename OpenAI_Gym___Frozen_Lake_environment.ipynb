{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI Gym | Frozen Lake environment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aicTQMm84kc3",
        "colab_type": "text"
      },
      "source": [
        "# The OpenAI Gym \n",
        "#### (Author : Soufiane Fadel)\n",
        "\\\\\n",
        "\n",
        "\n",
        "In order to download and install OpenAI Gym, you can use any of the following options:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSdmRUZy4J6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install gym[all]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmKRedA5QqK",
        "colab_type": "text"
      },
      "source": [
        "To understand the basics of importing Gym packages, loading an environment, and other important functions associated with OpenAI Gym, here's an example of a Frozen Lake environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35MdswQQ41jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make('FrozenLake-v0') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSrxeAbl6tyo",
        "colab_type": "text"
      },
      "source": [
        "## FrozenLake Game \n",
        "\n",
        "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "\n",
        "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
        "\n",
        "The surface is described using a grid like the following:\n",
        "\n",
        "* SFFF       (S: starting point, safe)\n",
        "* FHFH       (F: frozen surface, safe)\n",
        "* FFFH       (H: hole, fall to your doom)\n",
        "* HFFG       (G: goal, where the frisbee is located) \n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSprexh85fl7",
        "colab_type": "text"
      },
      "source": [
        "Next, we come to resetting the environment. While performing a reinforcement learning task, an agent undergoes learning through multiple episodes. As a result, at the start of each episode, the environment needs to be reset so that it comes to its initial situation and the agent begins from the start state. The following code shows the process for resetting an environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1QmiE0E6isi",
        "colab_type": "text"
      },
      "source": [
        "After taking each action, there might be a requirement to show the status of the agent in the environment. Visualizing that status is done by:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srXns-BY6PkK",
        "colab_type": "code",
        "outputId": "02f85190-2451-4c6a-b2fe-813f1b6ca6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrLsx0J5Xpd",
        "colab_type": "code",
        "outputId": "730c6bd6-38c5-4947-823a-c608fa7a2fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gym \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "s = env.reset()  # resets the environement and returns the start state as a value \n",
        "print(s)  # the initial state is 0 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRt-Vvdc8D3r",
        "colab_type": "text"
      },
      "source": [
        "In newer versions of the Gym, the environment features can't be modified directly. This is done by unwrapping the environment parameters with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYVr5_-F6nJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = env.unwrapped "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdtliWFF8ZPT",
        "colab_type": "text"
      },
      "source": [
        "Each environment is defined by the **state spaces** and **action spaces** for the agent to perform. The type (discrete or continuous) and size of state spaces and action spaces is very important to know in order to build a reinforcement learning agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk76r-g78M4S",
        "colab_type": "code",
        "outputId": "13fdfff4-1380-4265-8b38-a0ea760351d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(env.action_space)\n",
        "print(env.action_space.n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dixa47q188Cv",
        "colab_type": "text"
      },
      "source": [
        "The **Discrete(4)** output means that the action space of the Frozen Lake environment is a discrete set of values and has four distinct actions that can be performed by the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqPO3Nm081yZ",
        "colab_type": "code",
        "outputId": "1b29a2ea-22c5-49c6-d902-bade284c87e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.observation_space.n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16)\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XL8RQRN9KWi",
        "colab_type": "text"
      },
      "source": [
        "The **Discrete(16)** output means that the observation (state) space of the Frozen\n",
        "Lake environment is a discrete set of values and has 16 different states to be explored by the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih9MWnlp9W1E",
        "colab_type": "text"
      },
      "source": [
        "## Programming an agent for Frozen Lake Game with Q-Learning Epsilon-Greedy approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imhBOxH_9QVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2bAV7hyuDzV",
        "colab_type": "text"
      },
      "source": [
        "#### Load the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcovRHCxt894",
        "colab_type": "code",
        "outputId": "387f4214-27d8-4741-8608-b227bc86378c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "s = env.reset()\n",
        "print(s)\n",
        "env.render() # show the status of the agent in the environment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aay60VjJuo1g",
        "colab_type": "text"
      },
      "source": [
        "#### The epsilon greedy function  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq1403ckuoX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epsilon_greedy(Q,s,na):\n",
        "    epsilon = 0.3\n",
        "    p = np.random.uniform(low=0,high=1)\n",
        "    #print(p)\n",
        "    if p > epsilon:\n",
        "        return np.argmax(Q[s,:]) # say here,initial policy = for each state consider the action having highest Q-value\n",
        "    else:\n",
        "        return env.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8qDebo2vOcT",
        "colab_type": "text"
      },
      "source": [
        "#### Q-Learning Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRsSxrC5vL10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing Q-table with zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n]) \n",
        "\n",
        "# set hyperparameters\n",
        "lr = 0.5  # learning rate\n",
        "y = 0.9  # discount factor lambda\n",
        "eps = 600000  # total episodes being 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-azSU_v0QM",
        "colab_type": "text"
      },
      "source": [
        "###### Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lup8jlt-vgVV",
        "colab_type": "code",
        "outputId": "9d057efd-9900-42dd-9021-9d4e2cefd877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = time.time()\n",
        "\n",
        "for i in range(eps):\n",
        "    s = env.reset()\n",
        "    t = False\n",
        "    while(True):\n",
        "        a = epsilon_greedy(Q,s,env.action_space.n)\n",
        "        s_,r,t,_ = env.step(a)\n",
        "        if (r==0):  \n",
        "            if t==True:\n",
        "                r = -5 # to give negative rewards when holes turn up\n",
        "                Q[s_] = np.ones(env.action_space.n)*r    #in terminal state Q value equals the reward\n",
        "            else:\n",
        "                r = -1  # to give negative rewards to avoid long routes\n",
        "        if (r==1):\n",
        "                r = 100\n",
        "                Q[s_] = np.ones(env.action_space.n)*r    #in terminal state Q value equals the reward\n",
        "        Q[s,a] = Q[s,a] + lr * (r + y*np.max(Q[s_,a]) - Q[s,a])\n",
        "        s = s_   \n",
        "        if (t == True) :\n",
        "                break\n",
        "\n",
        "t2 = time.time()\n",
        "print(\"the training time for q-learning with epsillon greedy approach is : \" + str((t2-t1)/60) + \" min\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the training time for q-learning with epsillon greedy approach is : 3.2827404936154685 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rghoNiEJxL20",
        "colab_type": "text"
      },
      "source": [
        "#### Print the Q-Table "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_syZv54SwoEB",
        "colab_type": "code",
        "outputId": "ce063251-8758-4da5-8415-07c9ca9cef15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(\"Q-table\")\n",
        "print(Q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q-table\n",
            "[[ -9.87042831  -9.237709    -9.62332946 -10.        ]\n",
            " [ -9.65775364  -8.80562147  -9.5638128  -10.        ]\n",
            " [ -9.8090408   -7.78664567  -9.69939168 -10.        ]\n",
            " [ -9.63623497  -9.15642147  -9.62434233 -10.        ]\n",
            " [ -9.83454456  -9.34574071  -9.58726682  -9.74353066]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.78640909  -7.14843793  -9.55192421  -9.61975158]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.84750036  -7.707354    -9.53242899  -9.72817188]\n",
            " [ -9.73596282  -4.76495532  -9.21988037  -9.69629991]\n",
            " [ -9.8111297   -7.68835189  -9.59619696  -9.55662704]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -5.          -5.          -5.          -5.        ]\n",
            " [ -9.5982959   -3.66647969  18.3287359   -4.90939623]\n",
            " [ -9.7953489    4.08707219  47.86463047  14.4963989 ]\n",
            " [100.         100.         100.         100.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhzxe594xW3G",
        "colab_type": "text"
      },
      "source": [
        "#### Testing the trained agent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArpZdGhQxUzZ",
        "colab_type": "code",
        "outputId": "9ff53f25-d9d7-4b2c-c4db-8d1dc60130e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "s = env.reset()\n",
        "env.render()\n",
        "while(True):\n",
        "    a = np.argmax(Q[s])\n",
        "    s_,r,t,_ = env.step(a)\n",
        "    env.render()\n",
        "    s = s_\n",
        "    if(t==True) :\n",
        "        break        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73T2pQ_q1qoH",
        "colab_type": "text"
      },
      "source": [
        "## Programming an agent for Frozen Lake Game with Q-Network approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ho8x7c3xeDj",
        "colab_type": "code",
        "outputId": "96bc46a7-6f5e-482a-9e14-d39b0f3ff432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import time \n",
        "\n",
        "#Load the Environment\n",
        "env = gym.make('FrozenLake-v0')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMs-CrahhkQh",
        "colab_type": "text"
      },
      "source": [
        "#### Creating Neural Network and the defining the Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYzl37cGhjrL",
        "colab_type": "code",
        "outputId": "ffcf7fe4-bd37-490b-eef3-a50788efb1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#tensors for inputs, weights, biases, Qtarget\n",
        "inputs = tf.placeholder(shape=[None,env.observation_space.n],dtype=tf.float32)\n",
        "W = tf.get_variable(name=\"W\",dtype=tf.float32,shape=[env.observation_space.n,env.action_space.n],initializer=tf.contrib.layers.xavier_initializer())\n",
        "b = tf.Variable(tf.zeros(shape=[env.action_space.n]),dtype=tf.float32)\n",
        "qpred = tf.add(tf.matmul(inputs,W),b)\n",
        "apred = tf.argmax(qpred,1)\n",
        "\n",
        "qtar = tf.placeholder(shape=[1,env.action_space.n],dtype=tf.float32)\n",
        "loss = tf.reduce_sum(tf.square(qtar-qpred))\n",
        "\n",
        "train = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "minimizer = train.minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZoqNaFiB4X",
        "colab_type": "text"
      },
      "source": [
        "#### Training the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa2XEMGXiu0_",
        "colab_type": "text"
      },
      "source": [
        "###### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcPZENoyiBHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#learning parameters\n",
        "y = 0.5\n",
        "e = 0.3\n",
        "episodes = 100000\n",
        "\n",
        "#list to capture total steps and rewards per episodes\n",
        "slist = []\n",
        "rlist = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIW4Pxj3ihzy",
        "colab_type": "text"
      },
      "source": [
        "###### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey7bNu7Lilcq",
        "colab_type": "code",
        "outputId": "93499ddf-a1b4-461c-d3a1-8c334a3166a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = time.time()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(episodes):\n",
        "        s = env.reset() # resetting the environment at the start of each episode\n",
        "        r_total = 0  # to calculate the sum of rewards in the current episode\n",
        "        while(True):\n",
        "            #running the Q-network created above\n",
        "            a_pred,q_pred = sess.run([apred,qpred],feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1]})\n",
        "            #a_pred is the action prediction by the neural network\n",
        "            #q_pred contains q_values of the actions at current state 's'\n",
        "            if np.random.uniform(low=0,high=1) < e:\n",
        "                a_pred[0] = env.action_space.sample()\n",
        "                #exploring different action by randomly assigning them as the next action\n",
        "            s_,r,t,_ = env.step(a_pred[0])  #action taken and new state 's_' is encountered with a feedback reward 'r'\n",
        "            if r==0: \n",
        "                if t==True:\n",
        "                    r=-5  #if hole make the reward more negative\n",
        "                else:\n",
        "                    r=-1  #if block is fine/frozen then give slight negative reward to optimise the path\n",
        "            if r==1:\n",
        "                    r=5       #good positive goat state reward\n",
        "\n",
        "            q_pred_new = sess.run(qpred,feed_dict={inputs:np.identity(env.observation_space.n)[s_:s_+1]})\n",
        "            #q_pred_new contains q_values of the actions at the new state \n",
        "\n",
        "            #update the Q-target value for action taken\n",
        "            targetQ = q_pred\n",
        "            max_qpredn = np.max(q_pred_new)\n",
        "            targetQ[0,a_pred[0]] = r + y*max_qpredn\n",
        "            #this gives our targetQ\n",
        "\n",
        "            #train the neural network to minimise the loss\n",
        "            _ = sess.run(minimizer,feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1],qtar:targetQ})\n",
        "            r_total+=r\n",
        "\n",
        "            s=s_\n",
        "            if t==True:\n",
        "                break\n",
        "    \n",
        "# learning ends with the end of the loop of several episodes above\n",
        "    \n",
        "t2 = time.time()\n",
        "print(\"the training time for q-learning with epsillon greedy approach is : \" + str((t2-t1)/60) + \" min\" )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the training time for q-learning with epsillon greedy approach is : 70.54682284990946 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23FriM0DimDP",
        "colab_type": "text"
      },
      "source": [
        "###### Check how much our Q-network agent has learned "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdOtyEnpinx7",
        "colab_type": "code",
        "outputId": "1640f23c-3014-459e-d723-65607fb97bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  #learning ends with the end of the loop of several episodes above\n",
        "  #let's check how much our agent has learned\n",
        "  s = env.reset()\n",
        "  env.render()\n",
        "  while(True):\n",
        "      a = sess.run(apred,feed_dict={inputs:np.identity(env.observation_space.n)[s:s+1]})\n",
        "      s_,r,t,_ = env.step(a[0])\n",
        "      env.render()\n",
        "      s = s_\n",
        "      if t==True:\n",
        "          break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U1qGVBR4Pf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}